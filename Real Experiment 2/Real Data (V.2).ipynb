{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dfP = pd.read_csv(\"co2_327_45.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"Data preprocessing\n",
    "df = np.array(dfP)\n",
    "### \"Data preprocessing\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = 2\n",
    "r2 = 4\n",
    "r3 = 5\n",
    "p = 20\n",
    "N = 45\n",
    "Smp_size = 100\n",
    "Pred_size = len(df)-Smp_size-p-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from numpy import linalg as LA\n",
    "from time import perf_counter \n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "### '' Define the Network Structure: Linear\n",
    "### Type 1. Linear Network\n",
    "class NetLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, r1, r2, r3, p, N):\n",
    "        \n",
    "        self.r1 = r1\n",
    "        self.r2 = r2\n",
    "        self.r3 = r3\n",
    "        self.p = p\n",
    "        self.N = N\n",
    "        super(NetLinear, self).__init__()\n",
    "        # .conv1: 1 input matrix channel (N*P), r2 output channels, Nx1 convolution kernel\n",
    "        # .conv2: 1 input matrix channel (1*P), r3 output channels, 1xr3 convolution kernel (kernel sharing)\n",
    "        self.conv1 = nn.Conv2d(1, r2, kernel_size=(N, 1), bias=False) # stride is set to be (0,1) -> only move to the right\n",
    "        self.conv2 = nn.Conv2d(1, r3, kernel_size=(1, p), bias=False)   # stride is set to be 0 -> no moving needed\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(in_features=r2*r3, out_features=r1, bias=False)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(in_features=r1, out_features=N, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = self.conv1(x)\n",
    "        z = self.conv2(x[:, :1, :, :])\n",
    "        for i in range(1, x.shape[1]):\n",
    "            z = torch.cat([z, self.conv2(x[:, i:(i+1), :, :])], dim = 1) #Flattening is achieved     \n",
    "        z = z.view(-1, self.r2*self.r3) #-1 helps us figure out the batchsize\n",
    "        x = self.fc1(z) #activation can be added on the inside as well\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "### Type 2. Nonlinear Network\n",
    "class NetRelu(nn.Module):\n",
    "\n",
    "    def __init__(self, r1, r2, r3, p, N):\n",
    "        \n",
    "        self.r1 = r1\n",
    "        self.r2 = r2\n",
    "        self.r3 = r3\n",
    "        self.p = p\n",
    "        self.N = N\n",
    "        super(NetRelu, self).__init__()\n",
    "        # .conv1: 1 input matrix channel (N*P), r2 output channels, Nx1 convolution kernel\n",
    "        # .conv2: 1 input matrix channel (1*P), r3 output channels, 1xr3 convolution kernel (kernel sharing)\n",
    "        self.conv1 = nn.Conv2d(1, r2, kernel_size=(N, 1), bias=True) # stride is set to be (0,1) -> only move to the right\n",
    "        self.conv2 = nn.Conv2d(1, r3, kernel_size=(1, p), bias=True)   # stride is set to be 0 -> no moving needed\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(in_features=r2*r3, out_features=r1, bias=True)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(in_features=r1, out_features=N, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        z = F.relu(self.conv2(x[:, :1, :, :]))\n",
    "        for i in range(1, x.shape[1]):\n",
    "            z = torch.cat([z, self.conv2(x[:, i:(i+1), :, :])], dim = 1) #Flattening is achieved     \n",
    "        z = z.view(-1, self.r2*self.r3) #-1 helps us figure out the batchsize\n",
    "        x = self.fc1(z) #activation can be added on the inside as well\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "### Type 3. Nonlinear DW Network    \n",
    "class NetDW(nn.Module):\n",
    "\n",
    "    def __init__(self, r1, r2, r3, p, N):\n",
    "        \n",
    "        self.r1 = r1\n",
    "        self.r2 = r2\n",
    "        self.r3 = r3\n",
    "        self.p = p\n",
    "        self.N = N\n",
    "        super(NetDW, self).__init__()\n",
    "        # .conv1: 1 input matrix channel (N*P), r2 output channels, Nx1 convolution kernel\n",
    "        # .conv2: 1 input matrix channel (1*P), r3 output channels, 1xr3 convolution kernel (kernel sharing)\n",
    "        self.conv1 = nn.Conv2d(1, r2, kernel_size=(N, 1), bias=True) # stride is set to be (0,1) -> only move to the right\n",
    "        self.conv2 = nn.Conv2d(1, r3, kernel_size=(1, p), bias=True)   # stride is set to be 0 -> no moving needed\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(in_features=r2*r3, out_features=r1, bias=True)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(in_features=r1, out_features=N, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = F.relu(self.conv1(x))\n",
    "        y2 = F.relu(self.conv2(x))\n",
    "        # first N, then p\n",
    "        z1 = self.conv2(y1[:, :1, :, :])\n",
    "        for i in range(1, y1.shape[1]):\n",
    "            z1 = torch.cat([z1, self.conv2(y1[:, i:(i+1), :, :])], dim = 1) #Flattening is achieved     \n",
    "        z1 = F.relu(z1.view(-1, self.r2*self.r3)) #-1 helps us figure out the batchsize\n",
    "        # first p, then N\n",
    "        z2 = F.relu(self.conv1(y2[:, :1, :, :]))\n",
    "        for i in range(1, y2.shape[1]):\n",
    "            z2 = torch.cat([z2, self.conv1(y2[:, i:(i+1), :, :])], dim = 2) #Flattening is achieved     \n",
    "        z2 = F.relu(z2.view(-1, self.r2*self.r3)) #-1 helps us figure out the batchsize\n",
    "        x1 = self.fc2(F.relu(self.fc1(z1)))\n",
    "        x2 = self.fc2(F.relu(self.fc1(z2)))\n",
    "        x = torch.stack([x1,x2])\n",
    "        x = torch.mean(x,dim=0)\n",
    "        return x\n",
    "\n",
    "# Type 4: MLP\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, p, N):\n",
    "        self.p = p\n",
    "        self.N = N\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = N*p, out_features = N, bias = True)\n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        return y\n",
    "    \n",
    "# Type 5: RRR \n",
    "class RRR(nn.Module):\n",
    "    \n",
    "    def __init__(self, p, N, r1):\n",
    "        self.p = p\n",
    "        self.N = N\n",
    "        self.r1 = r1\n",
    "        super(RRR, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = N*p, out_features = r1, bias = True)\n",
    "        self.fc2 = nn.Linear(in_features = r1, out_features = N, bias = True)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        y = self.fc2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"Generate Linear Time Series Inputs and Targetted Output\n",
    "# return large transition Matrix A\n",
    "def kronecker(A, B):\n",
    "    return torch.ger(A.view(-1), B.view(-1)).reshape(*(A.size() + B.size())).permute([0, 2, 1, 3]).reshape(A.size(0)*B.size(0),A.size(1)*B.size(1))\n",
    "\n",
    "### \" F-norm of A\n",
    "class L2LossFun(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(L2LossFun, self).__init__()\n",
    "    def forward(self, A_Est, A_True):\n",
    "        gap = math.sqrt(torch.sum((A_Est - A_True)**2))\n",
    "        return gap\n",
    "\n",
    "class LinfLossFun(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LinfLossFun, self).__init__()\n",
    "    def forward(self, A_Est, A_True):\n",
    "        gap = max(abs(torch.squeeze(A_Est-A_True))).item()\n",
    "        return gap\n",
    "    \n",
    "def rearrangeG(K):\n",
    "    K = K.permute(1,0)\n",
    "    i = j = 0\n",
    "    for j in range(r3):\n",
    "            for i in range(r2):\n",
    "                if i == 0 and j == 0:\n",
    "                    tmp = K[:1,:]\n",
    "                else:\n",
    "                    tmp = torch.cat([tmp, K[(r3*i+j):(r3*i+1+j),:]], dim = 0)   \n",
    "    tmp = tmp.permute(1,0)\n",
    "    return(tmp)\n",
    "\n",
    "def Param_Matrix(net):\n",
    "    \n",
    "    U2T = torch.squeeze(net.conv1.weight)\n",
    "    U3T = torch.squeeze(net.conv2.weight)\n",
    "    G1 = rearrangeG(net.fc1.weight)\n",
    "    U1 = net.fc2.weight\n",
    "    A = torch.mm(torch.mm(U1, G1), kronecker(U3T,U2T))\n",
    "    \n",
    "    return A\n",
    "\n",
    "def Param_Dict(net):\n",
    "    \n",
    "    U2T = torch.squeeze(net.conv1.weight)\n",
    "    U3T = torch.squeeze(net.conv2.weight)\n",
    "    G1 = rearrangeG(net.fc1.weight)\n",
    "    U1 = net.fc2.weight\n",
    "    A = torch.mm(torch.mm(U1, G1), kronecker(U3T,U2T))\n",
    "    \n",
    "    Param_List = {\n",
    "        \"U2T\" : U2T,\n",
    "        \"U3T\" : U3T,\n",
    "        \"G1\" : G1,\n",
    "        \"U1\" : U1,\n",
    "        \"A\" : A\n",
    "    }\n",
    "    \n",
    "    return Param_List\n",
    "\n",
    "    #A = genA_true.genA(r1, r2, r3, p, N) #generate large transition matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \"We can use our method to generate RandomDataset\n",
    "# For our linear settings burnt in is needed\n",
    "class RealDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, p, N, Smp_size, df):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for i in range(Smp_size):\n",
    "            if i == 0:\n",
    "                input_TS = torch.tensor(df[ [19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0] , : ]).permute(1,0).view(1,1,N,p) #change according to p\n",
    "                self.X.append(torch.squeeze(input_TS.view(1,1,N,p), dim = 0))\n",
    "                output_TS = torch.squeeze(torch.tensor(df[ p:(p+1) , : ]).permute(1,0).view(1,1,N,1))\n",
    "                self.y.append(output_TS[:N])  \n",
    "            else:\n",
    "                input_TS = torch.cat([self.y[i-1].view(1,1,N,1), input_TS], dim = 3)\n",
    "                self.X.append(torch.squeeze(input_TS[:,:,:N,:p], dim = 0))\n",
    "                out_tmp = torch.squeeze(torch.tensor(df[ (i+p):(i+p+1) , : ]).permute(1,0).view(1,1,N,1))\n",
    "                self.y.append(out_tmp[:N])\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input to GPU\n",
    "#device = torch.device(\"cuda:2\")\n",
    "#device2 = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dict with dynamic list for storage\n",
    "names = {}\n",
    "\n",
    "names[\"y_true\"] = []\n",
    "\n",
    "names[\"predErrorDW\"] =  []\n",
    "names[\"predErrorDWLinf\"] =  []\n",
    "names[\"LTRDW_pred\"] = []\n",
    "names[\"predErrorN\"] =  []\n",
    "names[\"predErrorNLinf\"] =  []\n",
    "names[\"LTRN_pred\"] = []\n",
    "names[\"predErrorL\"] =  []\n",
    "names[\"predErrorLLinf\"] =  []\n",
    "names[\"LTRL_pred\"] = []\n",
    "names[\"predErrorM\"] =  []\n",
    "names[\"predErrorMLinf\"] =  []\n",
    "names[\"LTRM_pred\"] = []\n",
    "names[\"predErrorR\"] =  []\n",
    "names[\"predErrorRLinf\"] =  []\n",
    "names[\"LTRR_pred\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiqinghuang/.virtualenvs/dl4cv/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredError is 0.9595042759778677.\n",
      "PredErrorLinf is 0.4174957871437073.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiqinghuang/.virtualenvs/dl4cv/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredError Nonlinear is 0.9842291527134913.\n",
      "PredErrorLinf Nonlinear is 0.43367964029312134.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiqinghuang/.virtualenvs/dl4cv/lib/python3.6/site-packages/ipykernel_launcher.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredError Linear is 0.9567897569795436.\n",
      "PredErrorLinf Linear is 0.39854711294174194.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiqinghuang/.virtualenvs/dl4cv/lib/python3.6/site-packages/ipykernel_launcher.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredError MLP is 1.1841891965841351.\n",
      "PredErrorLinf MLP is 0.4802646040916443.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiqinghuang/.virtualenvs/dl4cv/lib/python3.6/site-packages/ipykernel_launcher.py:197: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredError RRR is 1.0168574558050556.\n",
      "PredErrorLinf RRR is 0.4326026439666748.\n",
      "0\n",
      "PredError is 1.200050041029555.\n",
      "PredErrorLinf is 0.5055556297302246.\n",
      "PredError Nonlinear is 1.2160351176649502.\n",
      "PredErrorLinf Nonlinear is 0.5289795994758606.\n",
      "PredError Linear is 1.3487390563351165.\n",
      "PredErrorLinf Linear is 0.5368069410324097.\n"
     ]
    }
   ],
   "source": [
    "distance = L2LossFun()\n",
    "distanceLinf = LinfLossFun()\n",
    "\n",
    "t1_start = perf_counter() \n",
    "\n",
    "for k in range(Pred_size):\n",
    "    \n",
    "    if k == 0:\n",
    "        dsT = RealDataset(p=p, N=N, Smp_size=Smp_size, df=df[k:,])\n",
    "    else:\n",
    "        dsT = dsF\n",
    "    \n",
    "    dsF = RealDataset(p=p, N=N, Smp_size=Smp_size, df=df[(k+1):,])\n",
    "    X_F,y_F = dsF[Smp_size-1]\n",
    "    X_F = X_F.view(1,1,N,p)\n",
    "    names[\"y_true\"].append(y_F)\n",
    "    ds = DataLoader(dsT, batch_size=Smp_size, shuffle=False)\n",
    "\n",
    "    netDW = NetDW(r1, r2, r3, p, N)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizerDW = optim.SGD(netDW.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "    loss_last = 1000\n",
    "    loss_new = 0\n",
    "    \n",
    "    ### \"\" netDW\n",
    "    i = 0\n",
    "    while abs(loss_last - loss_new) > 0.000001: \n",
    "        if i > 0:\n",
    "            loss_last = loss_new  \n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make inpur differentiable=======================\n",
    "            _x = Variable(_x).float()\n",
    "            _y = torch.squeeze(Variable(_y).float())\n",
    "            #========forward pass=====================================\n",
    "            yhat = netDW(_x).float()\n",
    "            loss = criterion(yhat, _y)\n",
    "            #=======backward pass=====================================\n",
    "            optimizerDW.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizerDW.step() \n",
    "            loss_new = loss.item()\n",
    "        i = i + 1\n",
    "        \n",
    "    X_F = Variable(X_F).float()\n",
    "    y_F = torch.tensor(Variable(y_F).float())\n",
    "    y_predDW = netDW(X_F).float()\n",
    "    predErrorDW = distance(y_predDW, y_F)\n",
    "    names[\"predErrorDW\"].append(predErrorDW)\n",
    "    print(\"PredError is {}.\".format(predErrorDW))\n",
    "    predErrorDWLinf = distanceLinf(y_predDW, y_F)\n",
    "    names[\"predErrorDWLinf\"].append(predErrorDWLinf)\n",
    "    print(\"PredErrorLinf is {}.\".format(predErrorDWLinf))\n",
    "    names[\"LTRDW_pred\"].append(y_predDW)\n",
    "    \n",
    "    ### \"\" netReLU\n",
    "    netN = NetRelu(r1, r2, r3, p, N)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizerN = optim.SGD(netN.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "    loss_last = 1000\n",
    "    loss_new = 0\n",
    "    i = 0\n",
    "    while abs(loss_last - loss_new) > 0.000001:\n",
    "        if i > 0:\n",
    "            loss_last = loss_new  \n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make inpur differentiable=======================\n",
    "            _x = Variable(_x).float()\n",
    "            _y = torch.squeeze(Variable(_y).float())\n",
    "            #========forward pass=====================================\n",
    "            yhat = netN(_x).float()\n",
    "            loss = criterion(yhat, _y)\n",
    "            #=======backward pass=====================================\n",
    "            optimizerN.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizerN.step() \n",
    "            loss_new = loss.item()\n",
    "        i = i + 1\n",
    "        \n",
    "    X_F = Variable(X_F).float()\n",
    "    y_F = torch.tensor(Variable(y_F).float())\n",
    "    y_predN = netN(X_F).float()\n",
    "    predErrorN = distance(y_predN, y_F)\n",
    "    names[\"predErrorN\"].append(predErrorN)\n",
    "    print(\"PredError Nonlinear is {}.\".format(predErrorN))\n",
    "    predErrorNLinf = distanceLinf(y_predN, y_F)\n",
    "    names[\"predErrorNLinf\"].append(predErrorNLinf)\n",
    "    print(\"PredErrorLinf Nonlinear is {}.\".format(predErrorNLinf))\n",
    "    names[\"LTRN_pred\"].append(y_predN)\n",
    "    \n",
    "    ### \"\" netLinear\n",
    "    netL = NetLinear(r1, r2, r3, p, N)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizerL = optim.SGD(netL.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "    loss_last = 1000\n",
    "    loss_new = 0\n",
    "    i = 0\n",
    "    while abs(loss_last - loss_new) > 0.000001:\n",
    "        if i > 0:\n",
    "            loss_last = loss_new  \n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make inpur differentiable=======================\n",
    "            _x = Variable(_x).float()\n",
    "            _y = torch.squeeze(Variable(_y).float())\n",
    "            #========forward pass=====================================\n",
    "            yhat = netL(_x).float()\n",
    "            loss = criterion(yhat, _y)\n",
    "            #=======backward pass=====================================\n",
    "            optimizerL.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizerL.step() \n",
    "            loss_new = loss.item()\n",
    "        i = i + 1\n",
    "        \n",
    "    X_F = Variable(X_F).float()\n",
    "    y_F = torch.tensor(Variable(y_F).float())\n",
    "    y_predL = netL(X_F).float()\n",
    "    predErrorL = distance(y_predL, y_F)\n",
    "    names[\"predErrorL\"].append(predErrorL)\n",
    "    print(\"PredError Linear is {}.\".format(predErrorL))\n",
    "    predErrorLLinf = distanceLinf(y_predL, y_F)\n",
    "    names[\"predErrorLLinf\"].append(predErrorLLinf)\n",
    "    print(\"PredErrorLinf Linear is {}.\".format(predErrorLLinf))\n",
    "    names[\"LTRL_pred\"].append(y_predL)\n",
    "    \n",
    "    ### \"\" MLP\n",
    "    netM = MLP(p, N)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizerM = optim.SGD(netM.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "    loss_last = 1000\n",
    "    loss_new = 0\n",
    "    i = 0\n",
    "    while abs(loss_last - loss_new) > 0.000001:\n",
    "        if i > 0:\n",
    "            loss_last = loss_new  \n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make inpur differentiable=======================\n",
    "            _x = torch.squeeze(_x.permute(0,1,3,2).reshape(Smp_size, 1, p*N).permute(1, 0, 2))\n",
    "            _x = Variable(_x).float()\n",
    "            _y = torch.squeeze(Variable(_y).float())\n",
    "            #========forward pass=====================================\n",
    "            yhat = netM(_x).float()\n",
    "            loss = criterion(yhat, _y)\n",
    "            #=======backward pass=====================================\n",
    "            optimizerM.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizerM.step() \n",
    "            loss_new = loss.item()\n",
    "        i = i + 1\n",
    "        \n",
    "    X_F = Variable(X_F.permute(0,1,3,2).reshape(1, p*N)).float()\n",
    "    y_F = torch.tensor(Variable(y_F).float())\n",
    "    y_predM = netM(X_F).float()\n",
    "    predErrorM = distance(y_predM, y_F)\n",
    "    names[\"predErrorM\"].append(predErrorM)\n",
    "    print(\"PredError MLP is {}.\".format(predErrorM))\n",
    "    predErrorMLinf = distanceLinf(y_predM, y_F)\n",
    "    names[\"predErrorMLinf\"].append(predErrorMLinf)\n",
    "    print(\"PredErrorLinf MLP is {}.\".format(predErrorMLinf))\n",
    "    names[\"LTRM_pred\"].append(y_predM)\n",
    "\n",
    "    ### \"\" RRR\n",
    "    netR = RRR(p, N, r1)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizerR = optim.SGD(netR.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "    loss_last = 1000\n",
    "    loss_new = 0\n",
    "    i = 0\n",
    "    while abs(loss_last - loss_new) > 0.000001:\n",
    "        if i > 0:\n",
    "            loss_last = loss_new  \n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make inpur differentiable=======================\n",
    "            _x = torch.squeeze(_x.permute(0,1,3,2).reshape(Smp_size, 1, p*N).permute(1, 0, 2))\n",
    "            _x = Variable(_x).float()\n",
    "            _y = torch.squeeze(Variable(_y).float())\n",
    "            #========forward pass=====================================\n",
    "            yhat = netR(_x).float()\n",
    "            loss = criterion(yhat, _y)\n",
    "            #=======backward pass=====================================\n",
    "            optimizerR.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizerR.step() \n",
    "            loss_new = loss.item()\n",
    "        i = i + 1\n",
    "        \n",
    "    X_F = Variable(X_F).float()\n",
    "    y_F = torch.tensor(Variable(y_F).float())\n",
    "    y_predR = netR(X_F).float()\n",
    "    predErrorR = distance(y_predR, y_F)\n",
    "    names[\"predErrorR\"].append(predErrorR)\n",
    "    print(\"PredError RRR is {}.\".format(predErrorR))\n",
    "    predErrorRLinf = distanceLinf(y_predR, y_F)\n",
    "    names[\"predErrorRLinf\"].append(predErrorRLinf)\n",
    "    print(\"PredErrorLinf RRR is {}.\".format(predErrorRLinf))\n",
    "    names[\"LTRR_pred\"].append(y_predR)\n",
    "\n",
    "    print(k)\n",
    "\n",
    "t1_stop = perf_counter() \n",
    "print(\"Elapsed time during the whole program in seconds:\", \n",
    "                                        t1_stop-t1_start) \n",
    "Real_100_245_20_2s = names\n",
    "torch.save(Real_100_245_20_2s, \"Real_100_245_20_2s.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7115289762922289\n",
      "0.3261041794504438\n",
      "0.7117751168962091\n",
      "0.326625252195767\n",
      "0.807227811429077\n",
      "0.326625252195767\n",
      "0.8024731564090585\n",
      "0.3356005276952471\n",
      "0.7673578566489941\n",
      "0.340144198565256\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean,median\n",
    "\n",
    "print(mean(names[\"predErrorDW\"]))\n",
    "print(mean(names[\"predErrorDWLinf\"]))\n",
    "print(mean(names[\"predErrorN\"]))\n",
    "print(mean(names[\"predErrorNLinf\"]))\n",
    "print(mean(names[\"predErrorL\"]))\n",
    "print(mean(names[\"predErrorLLinf\"]))\n",
    "print(mean(names[\"predErrorM\"]))\n",
    "print(mean(names[\"predErrorMLinf\"]))\n",
    "print(mean(names[\"predErrorR\"]))\n",
    "print(mean(names[\"predErrorRLinf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
